{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1VVxjuBnhuv51ETmgGgWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rghvsrdhtra/Multimodal_Training_Colab/blob/main/Multimodal_Training_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gpaar9P6F75"
      },
      "outputs": [],
      "source": [
        "# This notebook was designed as a flexible environment for exploring and comparing various model architectures.\n",
        "\n",
        "\n",
        "# --- Import Libraries ---\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from PIL import UnidentifiedImageError\n",
        "\n",
        "# ##############################################################################\n",
        "# ## âš™ï¸ 1. Configuration\n",
        "# ##############################################################################\n",
        "# --- Define all user-specific parameters here ---\n",
        "\n",
        "# === 1.1. Google Drive Paths ===\n",
        "# Name of your main project folder in Google Drive\n",
        "DRIVE_PROJECT_DIR_NAME = 'UPMC-FOOD101'\n",
        "# This will be constructed: '/content/drive/My Drive/YOUR_PROJECT_DIR_NAME'\n",
        "\n",
        "# === 1.2. Local Colab Environment ===\n",
        "# Path for temporary data storage on the Colab runtime\n",
        "LOCAL_DATA_PATH = '/content/data'\n",
        "\n",
        "# === 1.3. Data Structure ===\n",
        "# Names of your sub-folders in Google Drive\n",
        "IMAGE_DIR_NAME = 'images'\n",
        "TEXT_DIR_NAME = 'texts'\n",
        "\n",
        "# Names of your CSV files\n",
        "TRAIN_CSV_NAME = 'train_titles.csv'\n",
        "TEST_CSV_NAME = 'test_titles.csv'\n",
        "\n",
        "# === 1.4. CSV/DataFrame Structure ===\n",
        "# The column names in your CSV files\n",
        "FILENAME_COL = 'filename'\n",
        "TEXT_COL = 'title'\n",
        "LABEL_COL = 'label'\n",
        "\n",
        "# Set to True if your CSV files have a header row, False otherwise\n",
        "CSV_HAS_HEADER = False\n",
        "\n",
        "# === 1.5. Model Hyperparameters ===\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "DROPOUT_RATE = 0.5\n",
        "\n",
        "# === 1.6. Text Model Parameters ===\n",
        "MAX_TOKENS = 10000      # Max vocabulary size\n",
        "SEQUENCE_LENGTH = 20    # Max words per text input\n",
        "EMBEDDING_DIM = 128     # Dimension of the word embedding\n",
        "\n",
        "# === 1.7. Output Asset Names ===\n",
        "# Names for the files that will be saved back to your Google Drive\n",
        "MODEL_SAVE_NAME = 'best_multimodal_model.h5'\n",
        "VOCAB_SAVE_NAME = 'text_vectorizer_vocab.pkl'\n",
        "ACCURACY_PLOT_NAME = 'training_accuracy_plot.png'\n",
        "LOSS_PLOT_NAME = 'training_loss_plot.png'\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸš€ 2. Setup: Mount Drive & Define Paths\n",
        "# ##############################################################################\n",
        "\n",
        "print(\"Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Construct all paths based on configuration ---\n",
        "DRIVE_PROJECT_PATH = os.path.join('/content/drive/My Drive', DRIVE_PROJECT_DIR_NAME)\n",
        "\n",
        "# Source paths (in Google Drive)\n",
        "drive_images_path = os.path.join(DRIVE_PROJECT_PATH, IMAGE_DIR_NAME)\n",
        "drive_texts_path = os.path.join(DRIVE_PROJECT_PATH, TEXT_DIR_NAME)\n",
        "\n",
        "# Local paths (on Colab runtime)\n",
        "LOCAL_IMAGES_PATH = os.path.join(LOCAL_DATA_PATH, IMAGE_DIR_NAME)\n",
        "LOCAL_TEXTS_PATH = os.path.join(LOCAL_DATA_PATH, TEXT_DIR_NAME)\n",
        "\n",
        "train_img_dir = os.path.join(LOCAL_IMAGES_PATH, 'train')\n",
        "test_img_dir = os.path.join(LOCAL_IMAGES_PATH, 'test')\n",
        "train_csv_path = os.path.join(LOCAL_TEXTS_PATH, TRAIN_CSV_NAME)\n",
        "test_csv_path = os.path.join(LOCAL_TEXTS_PATH, TEST_CSV_NAME)\n",
        "\n",
        "print(f\"Project path set to: {DRIVE_PROJECT_PATH}\")\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸ“¥ 3. Copy Data from Drive to Local\n",
        "# ##############################################################################\n",
        "print(\"\\n--- Copying dataset from Google Drive to local Colab environment ---\")\n",
        "\n",
        "if os.path.exists(LOCAL_DATA_PATH):\n",
        "    print(\"Local data folder already exists. Skipping copy.\")\n",
        "else:\n",
        "    try:\n",
        "        shutil.copytree(drive_images_path, LOCAL_IMAGES_PATH)\n",
        "        shutil.copytree(drive_texts_path, LOCAL_TEXTS_PATH)\n",
        "        print(\"Dataset copied successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not copy data. Ensure Drive paths are correct.\")\n",
        "        print(f\"Details: {e}\")\n",
        "        # Stop execution if data copy fails\n",
        "        raise\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸ§¹ 4. Load and Preprocess DataFrames\n",
        "# ##############################################################################\n",
        "print(\"\\n--- Loading and Preparing Data from Local Colab Storage ---\")\n",
        "\n",
        "def load_dataframe(csv_path):\n",
        "    \"\"\"Loads a CSV file based on the configuration.\"\"\"\n",
        "    try:\n",
        "        if CSV_HAS_HEADER:\n",
        "            df = pd.read_csv(csv_path, header=0)\n",
        "        else:\n",
        "            col_names = [FILENAME_COL, TEXT_COL, LABEL_COL]\n",
        "            df = pd.read_csv(csv_path, names=col_names, header=None)\n",
        "\n",
        "        print(f\"Loaded {csv_path} with {len(df)} rows.\")\n",
        "        print(f\"Headers: {df.columns.tolist()}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Could not load local CSV file: {csv_path}.\")\n",
        "        print(f\"Details: {e}\")\n",
        "        return None\n",
        "\n",
        "train_df = load_dataframe(train_csv_path)\n",
        "test_df = load_dataframe(test_csv_path)\n",
        "\n",
        "if train_df is None:\n",
        "    raise ValueError(\"Failed to load training data. Stopping execution.\")\n",
        "\n",
        "# --- 4.1. Clean DataFrame ---\n",
        "def clean_df(df, img_dir):\n",
        "    \"\"\"\n",
        "    Cleans the DataFrame by creating a 'full_path' to the image\n",
        "    and removing rows where the image file does not exist.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return pd.DataFrame() # Return empty DF if loading failed\n",
        "\n",
        "    print(f\"\\nCleaning DataFrame for image directory: {img_dir}\")\n",
        "    # Create the full path using the configured column names\n",
        "    df['full_path'] = df.apply(\n",
        "        lambda row: os.path.join(img_dir, str(row[LABEL_COL]), str(row[FILENAME_COL])),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Check which files actually exist\n",
        "    mask = df['full_path'].apply(os.path.exists)\n",
        "    print(f\"Found {mask.sum()} existing files out of {len(df)} entries.\")\n",
        "\n",
        "    # Return only the rows with existing files\n",
        "    return df[mask]\n",
        "\n",
        "train_df = clean_df(train_df, train_img_dir)\n",
        "test_df = clean_df(test_df, test_img_dir)\n",
        "\n",
        "print(f\"Cleaned to {len(train_df)} training samples and {len(test_df)} testing samples.\")\n",
        "\n",
        "if len(train_df) == 0:\n",
        "    raise ValueError(\"Training dataframe is empty after cleaning. Check paths and filenames.\")\n",
        "\n",
        "# --- 4.2. Create Class Dictionaries ---\n",
        "class_names = sorted(train_df[LABEL_COL].unique())\n",
        "class_to_index = {name: i for i, name in enumerate(class_names)}\n",
        "index_to_class = {i: name for i, name in enumerate(class_names)}\n",
        "NUM_CLASSES = len(class_names)\n",
        "print(f\"\\nFound {NUM_CLASSES} classes.\")\n",
        "print(class_names)\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸ”  5. Text Vectorization\n",
        "# ##############################################################################\n",
        "print(\"\\n--- Setting up Text Vectorizer ---\")\n",
        "\n",
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_sequence_length=SEQUENCE_LENGTH,\n",
        "    standardize=\"lower_and_strip_punctuation\"\n",
        ")\n",
        "\n",
        "# Adapt the vectorizer to the training text\n",
        "# Ensure to use the correct text column and handle potential NaN values\n",
        "valid_train_titles = train_df[TEXT_COL].dropna()\n",
        "text_vectorizer.adapt(valid_train_titles)\n",
        "\n",
        "# --- Save the vocabulary ---\n",
        "vocab_save_path = os.path.join(DRIVE_PROJECT_PATH, VOCAB_SAVE_NAME)\n",
        "try:\n",
        "    with open(vocab_save_path, 'wb') as f:\n",
        "        pickle.dump(text_vectorizer.get_vocabulary(), f)\n",
        "    print(f\"Vocabulary saved successfully to: {vocab_save_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"WARNING: Could not save vocabulary. Details: {e}\")\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸ’¾ 6. Pre-load Data into Memory\n",
        "# ##############################################################################\n",
        "print(\"\\n--- Pre-loading all images and text into memory ---\")\n",
        "\n",
        "def load_data_into_memory(df):\n",
        "    \"\"\"Loads images, vectorizes text, and gets labels for a given DataFrame.\"\"\"\n",
        "    images = []\n",
        "    titles = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Loading data\"):\n",
        "        try:\n",
        "            # Load and process image\n",
        "            img = keras.preprocessing.image.load_img(\n",
        "                row['full_path'],\n",
        "                target_size=IMAGE_SIZE\n",
        "            )\n",
        "            img_array = keras.preprocessing.image.img_to_array(img)\n",
        "            images.append(img_array)\n",
        "\n",
        "            # Add text and label\n",
        "            titles.append(str(row[TEXT_COL])) # Ensure text is string\n",
        "            labels.append(class_to_index[row[LABEL_COL]])\n",
        "\n",
        "        except (UnidentifiedImageError, FileNotFoundError, IsADirectoryError) as e:\n",
        "            print(f\"\\nWARNING: Skipping corrupted or missing file: {row['full_path']}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"\\nWARNING: An unexpected error occurred for {row['full_path']}. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Vectorize all titles at once\n",
        "    vectorized_titles = text_vectorizer(np.array(titles))\n",
        "\n",
        "    return np.array(images), vectorized_titles, np.array(labels)\n",
        "\n",
        "X_train_images, X_train_text, y_train = load_data_into_memory(train_df)\n",
        "X_test_images, X_test_text, y_test = load_data_into_memory(test_df)\n",
        "\n",
        "print(f\"\\nSuccessfully loaded {len(X_train_images)} training samples and {len(X_test_images)} testing samples.\")\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸ“¦ 7. Create tf.data.Dataset Pipelines\n",
        "# ##############################################################################\n",
        "print(\"\\n--- Creating tf.data.Dataset pipelines ---\")\n",
        "\n",
        "def create_dataset(images, texts, labels):\n",
        "    \"\"\"Creates a batched and prefetched tf.data.Dataset.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        ({\"image_input\": images, \"text_input\": texts}, labels)\n",
        "    )\n",
        "    dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = create_dataset(X_train_images, X_train_text, y_train)\n",
        "print(\"Training dataset pipeline created.\")\n",
        "\n",
        "test_dataset = None\n",
        "if len(X_test_images) > 0:\n",
        "    test_dataset = create_dataset(X_test_images, X_test_text, y_test)\n",
        "    print(\"Test (validation) dataset pipeline created.\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No valid test data found. Validation will be skipped.\")\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸ¤– 8. Build the Multimodal Model\n",
        "# ##############################################################################\n",
        "print(\"\\n--- Building the Multimodal Model ---\")\n",
        "\n",
        "# --- 8.1. Image Branch (CNN) ---\n",
        "# Data augmentation\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1)\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "image_input = keras.Input(\n",
        "    shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n",
        "    name=\"image_input\"\n",
        ")\n",
        "augmented_image = data_augmentation(image_input)\n",
        "\n",
        "# Pre-trained base model\n",
        "# You can swap this with any keras.applications model (e.g., VGG16, ResNet50)\n",
        "base_model = keras.applications.EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling=\"avg\"\n",
        ")\n",
        "base_model.trainable = False # Freeze pre-trained weights\n",
        "image_features = base_model(augmented_image, training=False)\n",
        "image_features = layers.Dense(128, activation=\"relu\")(image_features)\n",
        "\n",
        "# --- 8.2. Text Branch (LSTM) ---\n",
        "text_input = keras.Input(\n",
        "    shape=(SEQUENCE_LENGTH,),\n",
        "    dtype=\"int32\",\n",
        "    name=\"text_input\"\n",
        ")\n",
        "text_features = layers.Embedding(\n",
        "    input_dim=MAX_TOKENS,\n",
        "    output_dim=EMBEDDING_DIM\n",
        ")(text_input)\n",
        "text_features = layers.LSTM(64)(text_features)\n",
        "text_features = layers.Dense(128, activation=\"relu\")(text_features)\n",
        "\n",
        "# --- 8.3. Fusion Branch ---\n",
        "combined_features = layers.concatenate([image_features, text_features])\n",
        "combined_features = layers.Dropout(DROPOUT_RATE)(combined_features)\n",
        "combined_features = layers.Dense(64, activation=\"relu\")(combined_features)\n",
        "output = layers.Dense(NUM_CLASSES, activation=\"softmax\")(combined_features)\n",
        "\n",
        "# --- 8.4. Create and Compile Model ---\n",
        "model = keras.Model(\n",
        "    inputs=[image_input, text_input],\n",
        "    outputs=output\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸš‚ 9. Train the Model\n",
        "# ##############################################################################\n",
        "\n",
        "# --- 9.1. Define Callbacks ---\n",
        "# Define the path to save the best model in your Google Drive\n",
        "best_model_save_path = os.path.join(DRIVE_PROJECT_PATH, MODEL_SAVE_NAME)\n",
        "\n",
        "# Monitor validation accuracy if available, otherwise training accuracy\n",
        "monitor_metric = 'val_accuracy' if test_dataset else 'accuracy'\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=best_model_save_path,\n",
        "    save_best_only=True,\n",
        "    monitor=monitor_metric,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    monitor=monitor_metric,\n",
        "    mode='max',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Starting Model Training (monitoring '{monitor_metric}') ---\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,  # This will be None if no test data\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
        ")\n",
        "\n",
        "print(\"--- Model Training Finished ---\")\n",
        "\n",
        "# ##############################################################################\n",
        "# ## ðŸ“Š 10. Save Results and Evaluate\n",
        "# ##############################################################################\n",
        "print(\"\\n--- Saving All Outputs Permanently to Google Drive ---\")\n",
        "\n",
        "# --- 10.1. Save Plots ---\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "def save_plot(history_df, metric_name, plot_save_path):\n",
        "    \"\"\"Generates and saves a training plot.\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot training metric\n",
        "    plt.plot(history_df[metric_name], label=f'Training {metric_name.capitalize()}')\n",
        "\n",
        "    # Plot validation metric if it exists\n",
        "    val_metric_name = f'val_{metric_name}'\n",
        "    if val_metric_name in history_df.columns:\n",
        "        plt.plot(history_df[val_metric_name], label=f'Validation {metric_name.capitalize()}')\n",
        "\n",
        "    plt.title(f'Model {metric_name.capitalize()} vs. Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(metric_name.capitalize())\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    try:\n",
        "        plt.savefig(plot_save_path)\n",
        "        print(f\"Successfully saved plot to {plot_save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not save plot. Details: {e}\")\n",
        "    plt.close()\n",
        "\n",
        "# Save Accuracy Plot\n",
        "accuracy_plot_path = os.path.join(DRIVE_PROJECT_PATH, ACCURACY_PLOT_NAME)\n",
        "save_plot(history_df, 'accuracy', accuracy_plot_path)\n",
        "\n",
        "# Save Loss Plot\n",
        "loss_plot_path = os.path.join(DRIVE_PROJECT_PATH, LOSS_PLOT_NAME)\n",
        "save_plot(history_df, 'loss', loss_plot_path)\n",
        "\n",
        "# --- 10.2. Evaluate Final Model ---\n",
        "# Note: The model already has its best weights restored by EarlyStopping\n",
        "if test_dataset:\n",
        "    print(\"\\n--- Evaluating Best Model on Test Data ---\")\n",
        "    loss, accuracy = model.evaluate(test_dataset)\n",
        "    print(f\"Final Test Accuracy: {accuracy*100:.2f}%\")\n",
        "    print(f\"Final Test Loss: {loss:.4f}\")\n",
        "else:\n",
        "    print(\"\\nSkipping final evaluation as no valid test data was found.\")\n",
        "\n",
        "print(f\"\\n--- All outputs saved to your Google Drive in '{DRIVE_PROJECT_DIR_NAME}'! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bm1HleVA6W4W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}